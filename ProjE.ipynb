{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os.path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# calculate accuracy\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_TRAIN_TRIPLES: 259\n"
     ]
    }
   ],
   "source": [
    "f_predpath = open('./Predicate_paths/city_capitals_1.csv','r')\n",
    "f_predpath = f_predpath.readlines()\n",
    "f_predpath = f_predpath[1:]\n",
    "f_predpath = [f_predpath[i].rstrip('\\n').split(',')[1:] for i in range(len(f_predpath))]\n",
    "for i in range(len(f_predpath)):\n",
    "    for j in range(len(f_predpath[0])):\n",
    "        if (f_predpath[i][j] == 'TRUE'): \n",
    "            f_predpath[i][j] = 1\n",
    "        if (f_predpath[i][j] == 'FALSE'):\n",
    "            f_predpath[i][j] = 0\n",
    "        f_predpath[i][j] = int(f_predpath[i][j])\n",
    "train_predpath = np.array(f_predpath)\n",
    "\n",
    "\n",
    "f_triple = open('./data_id/city_capital.csv','r')\n",
    "f_triple  = f_triple.readlines()\n",
    "f_triple = f_triple[1:]\n",
    "f_triple = [f_triple[i].rstrip('\\n').split(',')[:-1] for i in range(len(f_triple))]\n",
    "for i in range(len(f_triple)):\n",
    "    for j in range(len(f_triple[0])):\n",
    "        f_triple[i][j] = int(f_triple[i][j])\n",
    "f_triple = np.array(f_triple)\n",
    "hrt_triples = f_triple\n",
    "\n",
    "n_predicate = train_predpath.shape[1]-1\n",
    "print(\"N_TRAIN_TRIPLES: %d\" % train_predpath.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.1\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 100 # 1st layer number of neurons\n",
    "#n_hidden_2 = 1 # 2nd layer number of neurons\n",
    "n_input =  n_predicate# MNIST data input (img shape: 28*28)\n",
    "n_classes = 2 # MNIST total classes (0-9 digits)\n",
    "\n",
    "embed_dim = n_hidden_1\n",
    "bound = 6 / math.sqrt(n_hidden_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_uniform([n_input, n_hidden_1], minval=-bound, maxval=bound, seed=345)),\n",
    "    #'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_uniform([n_hidden_1, n_classes], minval=-bound, maxval=bound, seed=345))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros([n_hidden_1])),\n",
    "    #'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.zeros([n_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    #layer_1 = tf.matmul(x, weights['h1'])\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    #layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_1, weights['out'])  + biases['out']\n",
    "    #out_layer = tf.matmul(layer_1, weights['out']) \n",
    "    return out_layer\n",
    "\n",
    "# Construct model\n",
    "logits = multilayer_perceptron(X)\n",
    "    \n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing 10-folds training data...\n",
      "('Fold ', 1)\n",
      "('Label 1: ', 5, ' -- Label 0: ', 21)\n",
      "('Accuracy:', 0.88461536)\n",
      "('AUROC: ', 0.94285714285714284)\n",
      "('TP: ', 4, ', FN: ', 1, ', TN: ', 19, ', FP: ', 2)\n",
      "('Fold ', 2)\n",
      "('Label 1: ', 7, ' -- Label 0: ', 19)\n",
      "('Accuracy:', 0.92307693)\n",
      "('AUROC: ', 0.96240601503759393)\n",
      "('TP: ', 6, ', FN: ', 1, ', TN: ', 18, ', FP: ', 1)\n",
      "('Fold ', 3)\n",
      "('Label 1: ', 6, ' -- Label 0: ', 20)\n",
      "('Accuracy:', 0.88461536)\n",
      "('AUROC: ', 0.75)\n",
      "('TP: ', 3, ', FN: ', 3, ', TN: ', 20, ', FP: ', 0)\n",
      "('Fold ', 4)\n",
      "('Label 1: ', 5, ' -- Label 0: ', 21)\n",
      "('Accuracy:', 0.92307693)\n",
      "('AUROC: ', 0.95238095238095244)\n",
      "('TP: ', 5, ', FN: ', 0, ', TN: ', 19, ', FP: ', 2)\n",
      "('Fold ', 5)\n",
      "('Label 1: ', 4, ' -- Label 0: ', 22)\n",
      "('Accuracy:', 0.92307693)\n",
      "('AUROC: ', 0.85227272727272729)\n",
      "('TP: ', 3, ', FN: ', 1, ', TN: ', 21, ', FP: ', 1)\n",
      "('Fold ', 6)\n",
      "('Label 1: ', 7, ' -- Label 0: ', 19)\n",
      "('Accuracy:', 0.84615386)\n",
      "('AUROC: ', 0.75939849624060152)\n",
      "('TP: ', 4, ', FN: ', 3, ', TN: ', 18, ', FP: ', 1)\n",
      "('Fold ', 7)\n",
      "('Label 1: ', 3, ' -- Label 0: ', 23)\n",
      "('Accuracy:', 0.88461536)\n",
      "('AUROC: ', 0.93478260869565211)\n",
      "('TP: ', 3, ', FN: ', 0, ', TN: ', 20, ', FP: ', 3)\n",
      "('Fold ', 8)\n",
      "('Label 1: ', 3, ' -- Label 0: ', 23)\n",
      "('Accuracy:', 0.92307693)\n",
      "('AUROC: ', 0.66666666666666663)\n",
      "('TP: ', 1, ', FN: ', 2, ', TN: ', 23, ', FP: ', 0)\n",
      "('Fold ', 9)\n",
      "('Label 1: ', 6, ' -- Label 0: ', 20)\n",
      "('Accuracy:', 0.84615386)\n",
      "('AUROC: ', 0.84166666666666667)\n",
      "('TP: ', 5, ', FN: ', 1, ', TN: ', 17, ', FP: ', 3)\n",
      "('Fold ', 10)\n",
      "('Label 1: ', 4, ' -- Label 0: ', 21)\n",
      "('Accuracy:', 0.92000002)\n",
      "('AUROC: ', 0.85119047619047616)\n",
      "('TP: ', 3, ', FN: ', 1, ', TN: ', 20, ', FP: ', 1)\n",
      "('TP: ', 37, ', FN: ', 13, ', TN: ', 195, ', FP: ', 14)\n",
      "0.851362175201\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    #tf.global_variables_initializer().run()\n",
    "    kf = KFold(n_splits=10, shuffle = True, random_state=233)\n",
    "    print(\"Initializing 10-folds training data...\")\n",
    "    i_fold = 1\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    auc_score = 0.0\n",
    "    for i_train, i_test in kf.split(train_predpath):\n",
    "        print(\"Fold \", i_fold)  \n",
    "        tf.global_variables_initializer().run()\n",
    "        train_predicates = np.array(train_predpath)[i_train]\n",
    "        test_predicates = np.array(train_predpath)[i_test]\n",
    "        train_tiples = np.array(hrt_triples)[i_train]    \n",
    "        print(\"Label 1: \",np.sum(test_predicates[:,0]),\" -- Label 0: \", len(test_predicates[:,0])-np.sum(test_predicates[:,0]))\n",
    "        for epoch in range(training_epochs):\n",
    "\n",
    "            accu_loss = 0.\n",
    "            ninst = 0                 \n",
    "            #print(\"Minibatches training... iteration: \", n_iter)           \n",
    "            head_unique = np.unique(train_tiples[:,0])\n",
    "            for i_head in head_unique:\n",
    "                X_batch = train_predicates[train_tiples[:,0]==i_head,1:]\n",
    "                tmp = train_predicates[train_tiples[:,0]==i_head,0]\n",
    "                Y_labels = np.zeros([X_batch.shape[0], n_classes])\n",
    "                for j in range(X_batch.shape[0]):\n",
    "                    Y_labels[j,tmp[j]] = 1.\n",
    "                _, c = session.run([train_op, loss_op], feed_dict = \n",
    "                                   {X: X_batch, Y: Y_labels})\n",
    "                #accu_loss += l\n",
    "            #print(\"Loss \", accu_loss)\n",
    "        i_fold = i_fold + 1\n",
    "        \n",
    "        pred = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "        y_p = tf.argmax(pred, 1)\n",
    "        correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        tmp_ = test_predicates[:,0]\n",
    "        test_labels = np.zeros([len(tmp_), n_classes])\n",
    "        for j in range(len(tmp_)):\n",
    "            test_labels[j,tmp_[j]] = 1.\n",
    "        print(\"Accuracy:\", accuracy.eval({X: test_predicates[:,1:], Y: test_labels}))\n",
    "        y_pred = session.run([y_p], feed_dict={X: test_predicates[:,1:]})[0]\n",
    "        pred_proba = session.run([pred], feed_dict={X: test_predicates[:,1:]})[0]\n",
    "        y_true = np.argmax(test_labels,1)\n",
    "        TP_i = np.count_nonzero(y_pred * y_true)\n",
    "        TN_i = np.count_nonzero((y_pred - 1) * (y_true - 1))\n",
    "        FP_i = np.count_nonzero(y_pred * (y_true - 1))\n",
    "        FN_i = np.count_nonzero((y_pred - 1) * y_true)\n",
    "        auc_score += metrics.roc_auc_score(test_predicates[:,0], pred_proba[:, 1])\n",
    "        print(\"AUROC: \", metrics.roc_auc_score(test_predicates[:,0], pred_proba[:, 1]))\n",
    "        TP+=TP_i\n",
    "        TN+=TN_i\n",
    "        FP+=FP_i\n",
    "        FN+= FN_i\n",
    "        print(\"TP: \", TP_i,\", FN: \",FN_i,\", TN: \", TN_i,\", FP: \",FP_i)\n",
    "    print(\"TP: \", TP,\", FN: \",FN,\", TN: \", TN,\", FP: \",FP)\n",
    "    print auc_score/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing 10-folds training data...\n",
      "('Fold ', 1)\n",
      "('Label 1: ', 5, ' -- Label 0: ', 21)\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.780952380952\n",
      "('Fold ', 2)\n",
      "('Label 1: ', 7, ' -- Label 0: ', 19)\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.842105263158\n",
      "('Fold ', 3)\n",
      "('Label 1: ', 6, ' -- Label 0: ', 20)\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.841666666667\n",
      "('Fold ', 4)\n",
      "('Label 1: ', 5, ' -- Label 0: ', 21)\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.933333333333\n",
      "('Fold ', 5)\n",
      "('Label 1: ', 4, ' -- Label 0: ', 22)\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "1.0\n",
      "('Fold ', 6)\n",
      "('Label 1: ', 7, ' -- Label 0: ', 19)\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.842105263158\n",
      "('Fold ', 7)\n",
      "('Label 1: ', 3, ' -- Label 0: ', 23)\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.913043478261\n",
      "('Fold ', 8)\n",
      "('Label 1: ', 3, ' -- Label 0: ', 23)\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.695652173913\n",
      "('Fold ', 9)\n",
      "('Label 1: ', 6, ' -- Label 0: ', 20)\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.808333333333\n",
      "('Fold ', 10)\n",
      "('Label 1: ', 4, ' -- Label 0: ', 21)\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.785714285714\n",
      "[[ 200.    9.]\n",
      " [  18.   32.]]\n",
      "0.844290617849\n"
     ]
    }
   ],
   "source": [
    "    kf = KFold(n_splits=10, shuffle = True, random_state=233)\n",
    "    print(\"Initializing 10-folds training data...\")\n",
    "    i_fold = 1\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    Confusion_mat = np.zeros([2,2])\n",
    "    auc_score = 0.0\n",
    "    for i_train, i_test in kf.split(train_predpath):\n",
    "        print(\"Fold \", i_fold)  \n",
    "        train_predicates = np.array(train_predpath)[i_train]\n",
    "        test_predicates = np.array(train_predpath)[i_test]\n",
    "        train_tiples = np.array(hrt_triples)[i_train]    \n",
    "        print(\"Label 1: \",np.sum(test_predicates[:,0]),\" -- Label 0: \", len(test_predicates[:,0])-np.sum(test_predicates[:,0]))\n",
    "\n",
    "        logistic = LogisticRegression()\n",
    "        logistic.fit(train_predicates[:,1:], train_predicates[:,0])\n",
    "        print logistic\n",
    "        i_fold = i_fold + 1\n",
    "        \n",
    "        print(metrics.roc_auc_score(test_predicates[:,0], logistic.predict_proba(test_predicates[:,1:])[:,1]))\n",
    "        auc_score += metrics.roc_auc_score(test_predicates[:,0], logistic.predict_proba(test_predicates[:,1:])[:, 1])\n",
    "        Confusion_mat += metrics.confusion_matrix(test_predicates[:,0], logistic.predict(test_predicates[:,1:]))\n",
    "    print Confusion_mat\n",
    "    print auc_score/10\n",
    "                                                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.97976879e-01,   2.02312105e-03],\n",
       "       [  9.48479722e-01,   5.15202779e-02],\n",
       "       [  4.31231173e-02,   9.56876883e-01],\n",
       "       [  1.54102064e-10,   1.00000000e+00],\n",
       "       [  9.84247248e-01,   1.57527519e-02],\n",
       "       [  1.00000000e+00,   1.19303862e-10],\n",
       "       [  9.71466387e-01,   2.85336127e-02],\n",
       "       [  9.79940755e-01,   2.00592454e-02],\n",
       "       [  1.00000000e+00,   2.34271958e-22],\n",
       "       [  9.96812177e-01,   3.18782272e-03],\n",
       "       [  1.32890370e-01,   8.67109630e-01],\n",
       "       [  9.51230635e-01,   4.87693652e-02],\n",
       "       [  9.96726576e-01,   3.27342442e-03],\n",
       "       [  9.93972060e-01,   6.02794005e-03],\n",
       "       [  2.77708392e-01,   7.22291608e-01],\n",
       "       [  9.83800867e-01,   1.61991334e-02],\n",
       "       [  9.99918235e-01,   8.17651777e-05],\n",
       "       [  1.75674580e-01,   8.24325420e-01],\n",
       "       [  9.98502970e-01,   1.49702988e-03],\n",
       "       [  9.99912332e-01,   8.76677135e-05],\n",
       "       [  9.89522454e-01,   1.04775457e-02],\n",
       "       [  9.94863187e-01,   5.13681295e-03],\n",
       "       [  9.99999999e-01,   1.42096169e-09],\n",
       "       [  9.90457319e-01,   9.54268087e-03],\n",
       "       [  9.76295534e-01,   2.37044657e-02]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.predict_proba(test_predicates[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.predict(test_predicates[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_TRAIN_TRIPLES: 266\n"
     ]
    }
   ],
   "source": [
    "f_predpath = open('./Predicate_paths/capitals_3.csv','r')\n",
    "f_predpath = f_predpath.readlines()\n",
    "f_predpath = f_predpath[1:]\n",
    "f_predpath = [f_predpath[i].rstrip('\\n').split(',')[1:] for i in range(len(f_predpath))]\n",
    "for i in range(len(f_predpath)):\n",
    "    for j in range(len(f_predpath[0])):\n",
    "        if (f_predpath[i][j] == 'TRUE'): \n",
    "            f_predpath[i][j] = 1\n",
    "        if (f_predpath[i][j] == 'FALSE'):\n",
    "            f_predpath[i][j] = 0\n",
    "        f_predpath[i][j] = int(f_predpath[i][j])\n",
    "train_predpath = np.array(f_predpath)\n",
    "\n",
    "\n",
    "f_triple = open('./data_id/largest_city_capitals.csv','r')\n",
    "f_triple  = f_triple.readlines()\n",
    "f_triple = f_triple[1:]\n",
    "f_triple = [f_triple[i].rstrip('\\n').split(',')[1:-1] for i in range(len(f_triple))]\n",
    "for i in range(len(f_triple)):\n",
    "    for j in range(len(f_triple[0])):\n",
    "        f_triple[i][j] = int(f_triple[i][j])\n",
    "f_triple = np.array(f_triple)\n",
    "hrt_triples = f_triple\n",
    "\n",
    "n_predicate = train_predpath.shape[1]-1\n",
    "print(\"N_TRAIN_TRIPLES: %d\" % train_predpath.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1, 20, 17, ...,  0,  0,  0],\n",
       "       [ 1,  3,  8, ...,  0,  0,  0],\n",
       "       [ 1, 55, 61, ...,  0,  0,  0],\n",
       "       ..., \n",
       "       [ 0,  5,  6, ...,  0,  0,  0],\n",
       "       [ 0,  2,  2, ...,  0,  0,  0],\n",
       "       [ 0,  1,  4, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing 10-folds training data...\n",
      "('Fold ', 1)\n",
      "('Label 1: ', 5, ' -- Label 0: ', 22)\n",
      "0.7\n",
      "('Fold ', 2)\n",
      "('Label 1: ', 7, ' -- Label 0: ', 20)\n",
      "0.692857142857\n",
      "('Fold ', 3)\n",
      "('Label 1: ', 5, ' -- Label 0: ', 22)\n",
      "1.0\n",
      "('Fold ', 4)\n",
      "('Label 1: ', 4, ' -- Label 0: ', 23)\n",
      "0.945652173913\n",
      "('Fold ', 5)\n",
      "('Label 1: ', 7, ' -- Label 0: ', 20)\n",
      "0.921428571429\n",
      "('Fold ', 6)\n",
      "('Label 1: ', 6, ' -- Label 0: ', 21)\n",
      "0.801587301587\n",
      "('Fold ', 7)\n",
      "('Label 1: ', 4, ' -- Label 0: ', 22)\n",
      "1.0\n",
      "('Fold ', 8)\n",
      "('Label 1: ', 3, ' -- Label 0: ', 23)\n",
      "0.695652173913\n",
      "('Fold ', 9)\n",
      "('Label 1: ', 3, ' -- Label 0: ', 23)\n",
      "0.95652173913\n",
      "('Fold ', 10)\n",
      "('Label 1: ', 6, ' -- Label 0: ', 20)\n",
      "0.708333333333\n",
      "[[ 205.   11.]\n",
      " [  18.   32.]]\n",
      "0.842203243616\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=10, shuffle = True, random_state=233)\n",
    "print(\"Initializing 10-folds training data...\")\n",
    "i_fold = 1\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "Confusion_mat = np.zeros([2,2])\n",
    "auc_score = 0.0\n",
    "for i_train, i_test in kf.split(train_predpath):\n",
    "    print(\"Fold \", i_fold)  \n",
    "    train_predicates = np.array(train_predpath)[i_train]\n",
    "    test_predicates = np.array(train_predpath)[i_test]\n",
    "    train_tiples = np.array(hrt_triples)[i_train]    \n",
    "    print(\"Label 1: \",np.sum(test_predicates[:,0]),\" -- Label 0: \", len(test_predicates[:,0])-np.sum(test_predicates[:,0]))\n",
    "\n",
    "    logistic = LogisticRegression()\n",
    "    logistic.fit(train_predicates[:,1:], train_predicates[:,0])\n",
    "    i_fold = i_fold + 1\n",
    "\n",
    "    print(metrics.roc_auc_score(test_predicates[:,0], logistic.predict_proba(test_predicates[:,1:])[:,1]))\n",
    "    auc_score += metrics.roc_auc_score(test_predicates[:,0], logistic.predict_proba(test_predicates[:,1:])[:, 1])\n",
    "    Confusion_mat += metrics.confusion_matrix(test_predicates[:,0], logistic.predict(test_predicates[:,1:]))\n",
    "print Confusion_mat\n",
    "print auc_score/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88257575757575757"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "logreg = LogisticRegression()\n",
    "cross_val_score(logreg, np.array(train_predpath)[:,1:], np.array(train_predpath)[:,0], cv=10, scoring='roc_auc').mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 500\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 200 # 1st layer number of neurons\n",
    "#n_hidden_2 = 100 # 2nd layer number of neurons\n",
    "n_input =  n_predicate# MNIST data input (img shape: 28*28)\n",
    "n_classes = 2 # MNIST total classes (0-9 digits)\n",
    "\n",
    "embed_dim = n_hidden_1\n",
    "bound = 6 / math.sqrt(n_hidden_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_uniform([n_input, n_hidden_1], minval=-bound, maxval=bound, seed=345)),\n",
    "    #'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_uniform([n_hidden_1, n_classes], minval=-bound, maxval=bound, seed=345))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros([n_hidden_1])),\n",
    "    #'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.zeros([n_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    #layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.matmul(x, weights['h1'])\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    #layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    #out_layer = tf.matmul(layer_1, weights['out'])  + biases['out']\n",
    "    out_layer = tf.matmul(layer_1, weights['out']) \n",
    "    return out_layer\n",
    "\n",
    "# Construct model\n",
    "logits = multilayer_perceptron(X)\n",
    "    \n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing 10-folds training data...\n",
      "('Fold ', 1)\n",
      "('Label 1: ', 5, ' -- Label 0: ', 22)\n",
      "('Accuracy:', 0.8888889)\n",
      "('AUROC: ', 0.84090909090909083)\n",
      "('TP: ', 4, ', FN: ', 1, ', TN: ', 20, ', FP: ', 2)\n",
      "('Fold ', 2)\n",
      "('Label 1: ', 7, ' -- Label 0: ', 20)\n",
      "('Accuracy:', 0.85185188)\n",
      "('AUROC: ', 0.76428571428571435)\n",
      "('TP: ', 4, ', FN: ', 3, ', TN: ', 19, ', FP: ', 1)\n",
      "('Fold ', 3)\n",
      "('Label 1: ', 5, ' -- Label 0: ', 22)\n",
      "('Accuracy:', 0.8888889)\n",
      "('AUROC: ', 0.92727272727272725)\n",
      "('TP: ', 4, ', FN: ', 1, ', TN: ', 20, ', FP: ', 2)\n",
      "('Fold ', 4)\n",
      "('Label 1: ', 4, ' -- Label 0: ', 23)\n",
      "('Accuracy:', 0.8888889)\n",
      "('AUROC: ', 0.71739130434782616)\n",
      "('TP: ', 2, ', FN: ', 2, ', TN: ', 22, ', FP: ', 1)\n",
      "('Fold ', 5)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    #tf.global_variables_initializer().run()\n",
    "    kf = KFold(n_splits=10, shuffle = True, random_state=233)\n",
    "    print(\"Initializing 10-folds training data...\")\n",
    "    i_fold = 1\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    auc_score = 0.0\n",
    "    for i_train, i_test in kf.split(train_predpath):\n",
    "        print(\"Fold \", i_fold)  \n",
    "        tf.global_variables_initializer().run()\n",
    "        train_predicates = np.array(train_predpath)[i_train]\n",
    "        test_predicates = np.array(train_predpath)[i_test]\n",
    "        train_tiples = np.array(hrt_triples)[i_train]    \n",
    "        print(\"Label 1: \",np.sum(test_predicates[:,0]),\" -- Label 0: \", len(test_predicates[:,0])-np.sum(test_predicates[:,0]))\n",
    "        for epoch in range(training_epochs):\n",
    "\n",
    "            accu_loss = 0.\n",
    "            ninst = 0                 \n",
    "            #print(\"Minibatches training... iteration: \", n_iter)           \n",
    "            head_unique = np.unique(train_tiples[:,0])\n",
    "            for i_head in head_unique:\n",
    "                X_batch = train_predicates[train_tiples[:,0]==i_head,1:]\n",
    "                tmp = train_predicates[train_tiples[:,0]==i_head,0]\n",
    "                Y_labels = np.zeros([X_batch.shape[0], n_classes])\n",
    "                for j in range(X_batch.shape[0]):\n",
    "                    Y_labels[j,tmp[j]] = 1.\n",
    "                _, c = session.run([train_op, loss_op], feed_dict = \n",
    "                                   {X: X_batch, Y: Y_labels})\n",
    "                #accu_loss += l\n",
    "            #print(\"Loss \", accu_loss)\n",
    "        i_fold = i_fold + 1\n",
    "        \n",
    "        pred = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "        y_p = tf.argmax(pred, 1)\n",
    "        correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        tmp_ = test_predicates[:,0]\n",
    "        test_labels = np.zeros([len(tmp_), n_classes])\n",
    "        for j in range(len(tmp_)):\n",
    "            test_labels[j,tmp_[j]] = 1.\n",
    "        print(\"Accuracy:\", accuracy.eval({X: test_predicates[:,1:], Y: test_labels}))\n",
    "        y_pred = session.run([y_p], feed_dict={X: test_predicates[:,1:]})[0]\n",
    "        pred_proba = session.run([pred], feed_dict={X: test_predicates[:,1:]})[0]\n",
    "        y_true = np.argmax(test_labels,1)\n",
    "        TP_i = np.count_nonzero(y_pred * y_true)\n",
    "        TN_i = np.count_nonzero((y_pred - 1) * (y_true - 1))\n",
    "        FP_i = np.count_nonzero(y_pred * (y_true - 1))\n",
    "        FN_i = np.count_nonzero((y_pred - 1) * y_true)\n",
    "        auc_score += metrics.roc_auc_score(test_predicates[:,0], pred_proba[:, 1])\n",
    "        print(\"AUROC: \", metrics.roc_auc_score(test_predicates[:,0], pred_proba[:, 1]))\n",
    "        TP+=TP_i\n",
    "        TN+=TN_i\n",
    "        FP+=FP_i\n",
    "        FN+= FN_i\n",
    "        print(\"TP: \", TP_i,\", FN: \",FN_i,\", TN: \", TN_i,\", FP: \",FP_i)\n",
    "    print(\"TP: \", TP,\", FN: \",FN,\", TN: \", TN,\", FP: \",FP)\n",
    "    print auc_score/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
