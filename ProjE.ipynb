{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os.path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# calculate accuracy\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_TRAIN_TRIPLES: 259\n"
     ]
    }
   ],
   "source": [
    "f_predpath = open('./Predicate_paths/city_capitals_1.csv','r')\n",
    "f_predpath = f_predpath.readlines()\n",
    "f_predpath = f_predpath[1:]\n",
    "f_predpath = [f_predpath[i].rstrip('\\n').split(',')[1:] for i in range(len(f_predpath))]\n",
    "for i in range(len(f_predpath)):\n",
    "    for j in range(len(f_predpath[0])):\n",
    "        if (f_predpath[i][j] == 'TRUE'): \n",
    "            f_predpath[i][j] = 1\n",
    "        if (f_predpath[i][j] == 'FALSE'):\n",
    "            f_predpath[i][j] = 0\n",
    "        f_predpath[i][j] = int(f_predpath[i][j])\n",
    "train_predpath = np.array(f_predpath)\n",
    "\n",
    "\n",
    "f_triple = open('./data_id/city_capital.csv','r')\n",
    "f_triple  = f_triple.readlines()\n",
    "f_triple = f_triple[1:]\n",
    "f_triple = [f_triple[i].rstrip('\\n').split(',')[:-1] for i in range(len(f_triple))]\n",
    "for i in range(len(f_triple)):\n",
    "    for j in range(len(f_triple[0])):\n",
    "        f_triple[i][j] = int(f_triple[i][j])\n",
    "f_triple = np.array(f_triple)\n",
    "hrt_triples = f_triple\n",
    "\n",
    "n_predicate = train_predpath.shape[1]-1\n",
    "print(\"N_TRAIN_TRIPLES: %d\" % train_predpath.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.1\n",
    "training_epochs = 15\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 100 # 1st layer number of neurons\n",
    "#n_hidden_2 = 1 # 2nd layer number of neurons\n",
    "n_input =  n_predicate\n",
    "n_classes = 2\n",
    "\n",
    "embed_dim = n_hidden_1\n",
    "bound = 6 / math.sqrt(n_hidden_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_uniform([n_input, n_hidden_1], minval=-bound, maxval=bound, seed=345)),\n",
    "    #'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_uniform([n_hidden_1, n_classes], minval=-bound, maxval=bound, seed=345))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros([n_hidden_1])),\n",
    "    #'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.zeros([n_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    #layer_1 = tf.matmul(x, weights['h1'])\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    #layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_1, weights['out'])  + biases['out']\n",
    "    #out_layer = tf.matmul(layer_1, weights['out']) \n",
    "    return out_layer\n",
    "\n",
    "def trained_embedding(x):\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    return layer_1\n",
    "# Construct model\n",
    "logits = multilayer_perceptron(X)\n",
    "trained_new = trained_embedding(X)\n",
    "    \n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Neural Net: 10-folds Evaluation\n",
      "---Confusion Matrix---\n",
      "[[ 195.   14.]\n",
      " [  13.   37.]]\n",
      "---AUROC---\n",
      "0.851362175201\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    #tf.global_variables_initializer().run()\n",
    "    kf = KFold(n_splits=10, shuffle = True, random_state=233)\n",
    "    print(\"MLP Neural Net: 10-folds Evaluation\")\n",
    "    i_fold = 1\n",
    "    Confusion_mat = np.zeros([2,2])\n",
    "    auc_score = 0.0\n",
    "    for i_train, i_test in kf.split(train_predpath):\n",
    "        #print(\"Fold \", i_fold)  \n",
    "        tf.global_variables_initializer().run()\n",
    "        train_predicates = np.array(train_predpath)[i_train]\n",
    "        test_predicates = np.array(train_predpath)[i_test]\n",
    "        train_tiples = np.array(hrt_triples)[i_train]    \n",
    "        #print(\"Label 1: \",np.sum(test_predicates[:,0]),\" -- Label 0: \", len(test_predicates[:,0])-np.sum(test_predicates[:,0]))\n",
    "        for epoch in range(training_epochs):\n",
    "\n",
    "            accu_loss = 0.\n",
    "            ninst = 0                 \n",
    "            #print(\"Minibatches training... iteration: \", n_iter)           \n",
    "            head_unique = np.unique(train_tiples[:,0])\n",
    "            for i_head in head_unique:\n",
    "                X_batch = train_predicates[train_tiples[:,0]==i_head,1:]\n",
    "                tmp = train_predicates[train_tiples[:,0]==i_head,0]\n",
    "                Y_labels = np.zeros([X_batch.shape[0], n_classes])\n",
    "                for j in range(X_batch.shape[0]):\n",
    "                    Y_labels[j,tmp[j]] = 1.\n",
    "                _, c = session.run([train_op, loss_op], feed_dict = \n",
    "                                   {X: X_batch, Y: Y_labels})\n",
    "                #accu_loss += l\n",
    "            #print(\"Loss \", accu_loss)\n",
    "        i_fold = i_fold + 1\n",
    "        \n",
    "        pred = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "        y_p = tf.argmax(pred, 1)\n",
    "        correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        tmp_ = test_predicates[:,0]\n",
    "        test_labels = np.zeros([len(tmp_), n_classes])\n",
    "        for j in range(len(tmp_)):\n",
    "            test_labels[j,tmp_[j]] = 1.\n",
    "        #print(\"Accuracy:\", accuracy.eval({X: test_predicates[:,1:], Y: test_labels}))\n",
    "        y_pred = session.run([y_p], feed_dict={X: test_predicates[:,1:]})[0]\n",
    "        pred_proba = session.run([pred], feed_dict={X: test_predicates[:,1:]})[0]\n",
    "        y_true = np.argmax(test_labels,1)\n",
    "        auc_score += metrics.roc_auc_score(test_predicates[:,0], pred_proba[:, 1])\n",
    "        Confusion_mat += metrics.confusion_matrix(test_predicates[:,0], y_pred)\n",
    "    print \"---Confusion Matrix---\"\n",
    "    print(Confusion_mat)\n",
    "    print \"---AUROC---\"\n",
    "    print auc_score/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 10-folds Evaluation\n",
      "---Confusion Matrix---\n",
      "[[ 200.    9.]\n",
      " [  18.   32.]]\n",
      "---AUROC---\n",
      "0.844290617849\n"
     ]
    }
   ],
   "source": [
    "    kf = KFold(n_splits=10, shuffle = True, random_state=233)\n",
    "    print(\"Logistic Regression: 10-folds Evaluation\")\n",
    "    i_fold = 1\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    Confusion_mat = np.zeros([2,2])\n",
    "    auc_score = 0.0\n",
    "    for i_train, i_test in kf.split(train_predpath):\n",
    "        #print(\"Fold \", i_fold)  \n",
    "        train_predicates = np.array(train_predpath)[i_train]\n",
    "        test_predicates = np.array(train_predpath)[i_test]\n",
    "        train_tiples = np.array(hrt_triples)[i_train]    \n",
    "        #print(\"Label 1: \",np.sum(test_predicates[:,0]),\" -- Label 0: \", len(test_predicates[:,0])-np.sum(test_predicates[:,0]))\n",
    "\n",
    "        logistic = LogisticRegression()\n",
    "        logistic.fit(train_predicates[:,1:], train_predicates[:,0])\n",
    "        #print logistic\n",
    "        i_fold = i_fold + 1\n",
    "        \n",
    "        #print(metrics.roc_auc_score(test_predicates[:,0], logistic.predict_proba(test_predicates[:,1:])[:,1]))\n",
    "        auc_score += metrics.roc_auc_score(test_predicates[:,0], logistic.predict_proba(test_predicates[:,1:])[:, 1])\n",
    "        Confusion_mat += metrics.confusion_matrix(test_predicates[:,0], logistic.predict(test_predicates[:,1:]))\n",
    "    print \"---Confusion Matrix---\"\n",
    "    print(Confusion_mat)\n",
    "    print \"---AUROC---\"\n",
    "    print auc_score/10\n",
    "                                                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Neural Net: 10-folds Evaluation\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.1\n",
    "training_epochs = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 80 # 1st layer number of neurons\n",
    "#n_hidden_2 = 1 # 2nd layer number of neurons\n",
    "n_input =  n_predicate\n",
    "n_classes = 2\n",
    "\n",
    "embed_dim = n_hidden_1\n",
    "bound = 6 / math.sqrt(n_hidden_1)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_uniform([n_input, n_hidden_1], minval=-bound, maxval=bound, seed=345)),\n",
    "    #'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_uniform([n_hidden_1, n_classes], minval=-bound, maxval=bound, seed=345))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros([n_hidden_1])),\n",
    "    #'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.zeros([n_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    #layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.matmul(x, weights['h1'])\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    #layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    #out_layer = tf.matmul(layer_1, weights['out'])  + biases['out']\n",
    "    out_layer = tf.matmul(layer_1, weights['out']) \n",
    "    return out_layer\n",
    "\n",
    "def trained_embedding(x):\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    return layer_1\n",
    "# Construct model\n",
    "logits = multilayer_perceptron(X)\n",
    "trained_new = trained_embedding(X)\n",
    "    \n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    #tf.global_variables_initializer().run()\n",
    "    kf = KFold(n_splits=10, shuffle = True, random_state=233)\n",
    "    print(\"MLP Neural Net: 10-folds Evaluation\")\n",
    "    i_fold = 1\n",
    "    Confusion_mat = np.zeros([2,2])\n",
    "    auc_score = 0.0\n",
    "    for i_train, i_test in kf.split(train_predpath):\n",
    "        #print(\"Fold \", i_fold)  \n",
    "        tf.global_variables_initializer().run()\n",
    "        train_predicates = np.array(train_predpath)[i_train]\n",
    "        test_predicates = np.array(train_predpath)[i_test]\n",
    "        train_tiples = np.array(hrt_triples)[i_train]    \n",
    "        #print(\"Label 1: \",np.sum(test_predicates[:,0]),\" -- Label 0: \", len(test_predicates[:,0])-np.sum(test_predicates[:,0]))\n",
    "        for epoch in range(training_epochs):\n",
    "\n",
    "            accu_loss = 0.\n",
    "            ninst = 0                 \n",
    "            #print(\"Minibatches training... iteration: \", n_iter)           \n",
    "            head_unique = np.unique(train_tiples[:,0])\n",
    "            for i_head in head_unique:\n",
    "                X_batch = train_predicates[train_tiples[:,0]==i_head,1:]\n",
    "                tmp = train_predicates[train_tiples[:,0]==i_head,0]\n",
    "                Y_labels = np.zeros([X_batch.shape[0], n_classes])\n",
    "                for j in range(X_batch.shape[0]):\n",
    "                    Y_labels[j,tmp[j]] = 1.\n",
    "                _, c = session.run([train_op, loss_op], feed_dict = \n",
    "                                   {X: X_batch, Y: Y_labels})\n",
    "                #accu_loss += l\n",
    "            #print(\"Loss \", accu_loss)\n",
    "        i_fold = i_fold + 1\n",
    "        \n",
    "        pred = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "        y_p = tf.argmax(pred, 1)\n",
    "        correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        tmp_ = test_predicates[:,0]\n",
    "        test_labels = np.zeros([len(tmp_), n_classes])\n",
    "        for j in range(len(tmp_)):\n",
    "            test_labels[j,tmp_[j]] = 1.\n",
    "        #print(\"Accuracy:\", accuracy.eval({X: test_predicates[:,1:], Y: test_labels}))\n",
    "        y_pred = session.run([y_p], feed_dict={X: test_predicates[:,1:]})[0]\n",
    "        pred_proba = session.run([pred], feed_dict={X: test_predicates[:,1:]})[0]\n",
    "        y_true = np.argmax(test_labels,1)\n",
    "        auc_score += metrics.roc_auc_score(test_predicates[:,0], pred_proba[:, 1])\n",
    "        Confusion_mat += metrics.confusion_matrix(test_predicates[:,0], y_pred)\n",
    "    print \"---Confusion Matrix---\"\n",
    "    print(Confusion_mat)\n",
    "    print \"---AUROC---\"\n",
    "    print auc_score/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
